{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:      Stock_Index_Price   R-squared:                       0.898\n",
      "Model:                            OLS   Adj. R-squared:                  0.888\n",
      "Method:                 Least Squares   F-statistic:                     92.07\n",
      "Date:                Thu, 26 Aug 2021   Prob (F-statistic):           4.04e-11\n",
      "Time:                        18:51:41   Log-Likelihood:                -134.61\n",
      "No. Observations:                  24   AIC:                             275.2\n",
      "Df Residuals:                      21   BIC:                             278.8\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const              1798.4040    899.248      2.000      0.059     -71.685    3668.493\n",
      "Interest_Rate       345.5401    111.367      3.103      0.005     113.940     577.140\n",
      "Unemployment_Rate  -250.1466    117.950     -2.121      0.046    -495.437      -4.856\n",
      "==============================================================================\n",
      "Omnibus:                        2.691   Durbin-Watson:                   0.530\n",
      "Prob(Omnibus):                  0.260   Jarque-Bera (JB):                1.551\n",
      "Skew:                          -0.612   Prob(JB):                        0.461\n",
      "Kurtosis:                       3.226   Cond. No.                         394.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "Stock_Market = {'Year': [2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016],\n",
    "                'Month': [12, 11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1],\n",
    "                'Interest_Rate': [2.75,2.5,2.5,2.5,2.5,2.5,2.5,2.25,2.25,2.25,2,2,2,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75],\n",
    "                'Unemployment_Rate': [5.3,5.3,5.3,5.3,5.4,5.6,5.5,5.5,5.5,5.6,5.7,5.9,6,5.9,5.8,6.1,6.2,6.1,6.1,6.1,5.9,6.2,6.2,6.1],\n",
    "                'Stock_Index_Price': [1464,1394,1357,1293,1256,1254,1234,1195,1159,1167,1130,1075,1047,965,943,958,971,949,884,866,876,822,704,719]        \n",
    "                }\n",
    "\n",
    "df = DataFrame(Stock_Market,columns=['Year','Month','Interest_Rate','Unemployment_Rate','Stock_Index_Price']) \n",
    "\n",
    "X = df[['Interest_Rate','Unemployment_Rate']]\n",
    "Y = df['Stock_Index_Price']\n",
    "\n",
    "X = sm.add_constant(X) \n",
    "\n",
    "model = sm.OLS(Y, X).fit()\n",
    "predictions = model.predict(X) \n",
    "\n",
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,)\n",
      "(24, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(X.shape)\n",
    "round(4.04e-11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant Term : Intercept of the regression line\n",
    "What is the need for such is brought brings our mind to question.\n",
    "The average  of the omitted independent variables and noise present in the model can be put to question through the adding\n",
    "of the constant.It serves as a garbage bin for any bias that is not accounted for by the terms in the model.\n",
    "The constant guarantees that the residuals don’t have an overall positive or negative bias, \n",
    "but also makes it harder to interpret the value of the constant because it absorbs the bias.\n",
    "Also known as the y intercept.It guarantees that your residuals have a mean of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF Residuals:Degrees of freedom are the number of independent values that a statistical analysis can estimate. \n",
    "You can also think of it as the number of values that are free to vary as you estimate parameters.\n",
    "Degrees of freedom encompasses the notion that the amount of independent information you have \n",
    "limits the number of parameters that you can estimate.\n",
    "It actually indicates how much independent information goes into the estimate.\n",
    "n-k+1[k is the total number of independent variables,n is the number of observations]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-Squared -It tells us how much variations we have in the \n",
    "dependent variable with the help of X variable or the Independent variable.\n",
    "The higher the R-squared value, the better it is.\n",
    "If we keep increasing the variables in the model, the R-squared will increase.\n",
    "The amount of signifance derived from the R-Squared is not entirely captured.\n",
    "if we drive further into this, we can figure out that even if we see on further analysis that few of the existing variables or added further can have a non linear relationship with  the dependent variaable.Even then, \n",
    "it makes no significant partition or reduces with the presence of non-linearity.\n",
    "So the add on of the variables must show a decrease in the R-Squared, but it doesn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adj R-squared- Now, it solves the problem of the R-Squared.\n",
    "How?\n",
    "We can see that the score of the Adj R- Squared decreases. It can give a subtle hint \n",
    "on the influence of the independent variables that are taken into consideration with respect to linearity.\n",
    "If you keep on adding features that are redundant and do not actively help in explaining the model, the adjusted R-Squared model would drastically go down to indicate the intrusion of those models.It's a sign that a variable might not be relevant to your model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-Statistic-It helps to derive the overall significance of the model.In case of multiple regression, like the one we have here, it compares the model with no predictors. It is intercept model only.The null hypothesis in this case is that both the models are same, the baseline or intercept model and the model with predictors and the alternate hypothesis model states that the intercept only model is worse than this model. We will get back a p-value as well as a statistic value that will help you choose whether to accept or reject the null hypothesis value.Now what does this say about the prediction?\n",
    "Let's say that we build a model, when we call that a baseline model, we create the model with the intercept only and nothing else,like a straight line parallel to X-axis.\n",
    "Through the base line model, whatever error we get, we can achieve the MSE[Mean Square Error]. Let's say we are getting a Mean Square Error of around 250\n",
    "So this will be our base line model, after that we create the Linear Regression model with the help of all the independent variables.\n",
    "So, our error should always be less than our base line model,only then we can say that our model fits good or the model performs better than our baseline model.\n",
    "We use the hypothesis testing here.We have to check the null hypothesis.Here with F-Statistic,the null hypothesis ststes that the model with no independent variables\n",
    "fits the data as well as the model.So the MSE will be the null hypothesis[MSE= 250]\n",
    "But the alternativ hypothesis states that the model fits the data better than the intercept only model[MSE<250],we accept the alternate hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prob (F-Statistic)-It tells us what is the accuracy of getting the null hypothesis. If we get [MSE>250],then we can say that accuracy is around 100%\n",
    "If we get [MSE < 250], we get the accuracy of o%.\n",
    "This number tells us the accuracy of the null hypothesis, or whether it is accurate that our variables’ effect is 0. In this case, it is telling us 0.0% chance of this.\n",
    "round(4.04e-11) = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-test- T-test behaves slightly different from the F-test. It looks at the atrget variable and the predictor variable independently without taking into account all the features at once. The null hypothesis of a particular feature is that the feature value is going to be zero. The alternate coefficient value is that it is not equal to zero. So given this, we have T-values and P-values associated with the T-test. higher the T-value, the greater the chances that one rejects the null hypothesis and accept the alternate hypothesis. Lower the values of P, it again signifies that one should reject thenull hypothesis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.025 and 0.975] : are both measurements of values of our coefficients within 95% of our data, or within two standard deviations. Outside of these values can generally be considered outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-Liklihood-Likelihood Ratio test (often termed as LR test) is a test to compare two models, concentrating on the improvement with respect to likelihood value. If we keep on adding predictor variables to a linear model, R-square will improve. This holds true for model likelihood value as well. But, the objective is to check if the improvement in likelihood is good enough or not!\n",
    "\n",
    "we have a data set with two variables, X and Y. You have fitted a regression equation between the two and got estimates or coefficients. Now, the likelihood is a measure that tells you how likely is that you will get a dataset like what you have, given the regression equation.\n",
    "\n",
    "So, higher the value of likelihood, better is the fit of the model. once a regression model is fit, we may like to measure the likelihood of the estimates, for which we look at the log of the likelihood value and call it Log Likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t : is related and is a measurement of the precision with which the coefficient was measured. A low std error compared to a high coefficient produces a high t statistic, which signifies a high significance for your coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P>|t| : is one of the most important statistics in the summary. The p-value for each independent variable tests the null hypothesis that the variable has no correlation with the dependent variable. If there is no correlation, there is no association between the changes in the independent variable and the shifts in the dependent variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
