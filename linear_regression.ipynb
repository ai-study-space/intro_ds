{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3f19ff-896c-4407-a80a-00885903bc8f",
   "metadata": {},
   "source": [
    "### Different components of a simple linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29edc2c3-3a23-4147-8918-8060cbf6bb16",
   "metadata": {},
   "source": [
    "In Ordinary Least Squares (OLS) Linear Regression, our goal is to define the best-fitting line with following equation:\n",
    "\n",
    "<img src=\"images/ols_equation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba3940a-486d-4132-ab07-fdac50f9b17d",
   "metadata": {},
   "source": [
    "That is we have to find the line (or hyperplane) that minimizes the vertical offsets (the `red` lines in the following figure)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f22c4-29ff-4338-9dc0-ba632496d13e",
   "metadata": {},
   "source": [
    "<img src=\"images/ols_described_in_graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cbb402-a14d-4eb4-95f7-3807b95e8f60",
   "metadata": {},
   "source": [
    "The best fitting line minimizes the `sum of squared errors` (**SSE**) or `mean squared error` (**MSE**) between the target variable ($y$) and the predicted output over all samples $i$ in our dataset of size $n$.\n",
    "\n",
    "<img src=\"images/ols_sse_mse.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad3bf9-f3be-467a-a86a-528cce72d068",
   "metadata": {},
   "source": [
    "Since `linear regression` is the process of fitting the typical linear hypothesis, this can be achieved using one of the following approaches:\n",
    "\n",
    "- Solving the model parameters analytically using least-square loss and normal equations (closed-form equations) method. (e.g. class <span style=\"background-color:#CBFF33\"> LinearRegression </span>)\n",
    "- Applying an optimization algorithm such as Gradient Descent, Stochastic Gradient Descent, Newton's Method, Simplex Method, etc. (e.g. class <span style=\"background-color:#CBFF33\"> SGDRegressor </span>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1b6b2e-dea5-4ddc-b96b-38981f387865",
   "metadata": {},
   "source": [
    "### Gradient Descent (GD) based optimization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac60253-0d1c-488f-bc12-fd841a300e49",
   "metadata": {},
   "source": [
    "- `SGDRegressor` wherein we specify the specific `loss function` and penalty and it uses `stochastic gradient descent (SGD)` to do the fitting.  \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>SGD:</b> In SGD we repeatedly run through the training set <span style=\"background-color:#33ECFF\"> one data point at a time </span> and update the parameters according to the gradient of the error with respect to each individual data point.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>SGDRegressor:</b> Linear model fitted by minimizing a regularized empirical loss with SGD.\n",
    "</div>\n",
    "\n",
    "Be sure to set the $n_{iter}$ parameter high enough to get good convergence. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b579a94-7df9-4533-badd-63b8d4bf2635",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Let's assume a cost function with `more than one minima`.\n",
    "In this scenario how the regular batch GD and SGD will behave?\n",
    "- The regular batch GD will walk towards one `minima` and it may happen to be the local minima and not the global minima.\n",
    "- However, since SGD tend to jump all over the places due to the randomness, it may end up falling across minimum regions and not blined by one minima. This is one of the prominent a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5094c8f-2916-4528-957c-227c58bf8f09",
   "metadata": {},
   "source": [
    "### References\n",
    "- This notebook is based on the follwing web resources:\n",
    "    - this [quora discussion](https://qr.ae/pG9WVc)\n",
    "    - this [blog post](https://sdsawtelle.github.io/blog/output/week2-andrew-ng-machine-learning-with-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e082776f-cbc9-4be3-af8c-3acea762787a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Visualizing regression models](https://seaborn.pydata.org/tutorial/regression.html) a good place to see the fitted lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac5b8ce-2e8a-4216-bfbb-ef6fb254d3f1",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
